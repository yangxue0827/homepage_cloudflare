<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>ğŸ§‘â€ğŸ’» Selected Publications | Cloudflare</title>
    <meta name="description" content="A academic homepage">
    <meta name="generator" content="VitePress v1.3.4">
    <link rel="preload stylesheet" href="/assets/style.CLoQHaoc.css" as="style">
    <script type="module" src="/assets/chunks/metadata.8f03b390.js"></script>
    <script type="module" src="/assets/app.D3d27I8Z.js"></script>
    <link rel="modulepreload" href="/assets/chunks/theme.tQvbJa1c.js">
    <link rel="modulepreload" href="/assets/chunks/framework.B2E0Tgba.js">
    <link rel="modulepreload" href="/assets/publication.md.B9SskSYW.lean.js">
    <link rel="preconnect" href="https://api.fontshare.com">
    <link rel="stylesheet" href="https://api.fontshare.com/v2/css?f[]=supreme@300,301,400,401,500,501,700,701,800,801,1,2&amp;display=swap">
    <link rel="stylesheet" href="https://api.fontshare.com/v2/css?f[]=sentient@300,301,400,401,500,501,700,701&amp;display=swap">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><!--[--><div class="n-config-provider"><div class="Layout"><div class="__layout-light-1heifd0 n-layout n-layout--static-positioned h-screen w-screen min-h-full" style=""><div class="n-layout-scroll-container" style="display:flex;flex-wrap:nowrap;width:100%;flex-direction:row;"><aside class="n-layout-sider __layout-sider-light-1heifd0-b n-layout-sider--static-positioned n-layout-sider--left-placement n-layout-sider--bordered n-layout-sider--show-content" style="max-width:300px;width:300px;"><div class="n-layout-sider-scroll-container" style="min-width:300px;overflow:auto;padding:24px;"><!--[--><div class="site-name text-center text-3xl font-bold text-amber-800"> DeepVision Lab </div><div class="flex flex-col p-4 pt-12 w-full"><div class="APSideCardInfo flex flex-col my-4 justify-around items-center"><!--[--><div class="APSideCardInfoAvatar round"><span class="n-avatar __avatar-light-1heifd0-a180bc" style=""><img loading="eager" src="/Wei-Shen.jpg" data-image-src="/Wei-Shen.jpg" style=""><!----></span></div><div class="APSideCardInfoName pt-3"><h2 style="color:#000;line-height:1rem;">Author Name</h2></div><div class="APSideCardInfoTags flex flex-col items-center justify-center gap-2"><!--[--><div class="n-tag __tag-light-1heifd0-im n-tag--round w-max" style=""><!----><span class="n-tag__content">Professor</span><!----><!----></div><div class="n-tag __tag-light-1heifd0-im n-tag--round w-max" style=""><!----><span class="n-tag__content">PhD Supervisor</span><!----><!----></div><div class="n-tag __tag-light-1heifd0-im n-tag--round w-max" style=""><!----><span class="n-tag__content">Shanghai Jiao Tong University</span><!----><!----></div><!--]--></div><div class="APSideCardInfoLine w-full"><div role="separator" class="n-divider __divider-light-1heifd0 n-divider--no-title" style="margin-top:16px;margin-bottom:10px;background-color:rgb(239, 239, 245);"><div class="n-divider__line n-divider__line--left"></div><!----></div></div><!--]--></div><div class="APSideCardLinks flex flex-row mx-auto gap-6"><!--[--><section class="flex justify-center items-center"><a href="mailto:huiser@huiserwang.site" class="group flex justify-center p-2 rounded-md drop-shadow-xl bg-gradient-to-r from-stone-100 to-stone-100 text-white font-semibold hover:translate-y-3 hover:rounded-[50%] transition-all duration-500 hover:from-[#fefefe] hover:to-[#f9f9f9]"><svg xmlns="http://www.w3.org/2000/svg" width="1.5rem" height="1.5rem" role="img" viewBox="0 0 32 32" fill="currentColor" stroke-width="0" stroke="currentColor"><g transform="scale(1.5) translate(-5, -5)"><path d="M22.0515 8.52295L16.0644 13.1954L9.94043 8.52295V8.52421L9.94783 8.53053V15.0732L15.9954 19.8466L22.0515 15.2575V8.52295Z" fill="#EA4335"></path><path d="M23.6231 7.38639L22.0508 8.52292V15.2575L26.9983 11.459V9.17074C26.9983 9.17074 26.3978 5.90258 23.6231 7.38639Z" fill="#FBBC05"></path><path d="M22.0508 15.2575V23.9924H25.8428C25.8428 23.9924 26.9219 23.8813 26.9995 22.6513V11.459L22.0508 15.2575Z" fill="#34A853"></path><path d="M9.94811 24.0001V15.0732L9.94043 15.0669L9.94811 24.0001Z" fill="#C5221F"></path><path d="M9.94014 8.52404L8.37646 7.39382C5.60179 5.91001 5 9.17692 5 9.17692V11.4651L9.94014 15.0667V8.52404Z" fill="#C5221F"></path><path d="M9.94043 8.52441V15.0671L9.94811 15.0734V8.53073L9.94043 8.52441Z" fill="#C5221F"></path><path d="M5 11.4668V22.6591C5.07646 23.8904 6.15673 24.0003 6.15673 24.0003H9.94877L9.94014 15.0671L5 11.4668Z" fill="#4285F4"></path></g></svg><span class="absolute opacity-0 group-hover:opacity-100 group-hover:text-gray-700 group-hover:text-sm group-hover:-translate-y-10 duration-700">Email</span></a></section><section class="flex justify-center items-center"><a class="group flex justify-center p-2 rounded-md drop-shadow-xl text-white font-semibold hover:translate-y-3 hover:rounded-[50%] transition-all duration-500 hover:from-[#331029] hover:to-[#310413] bg-gradient-to-r from-gray-800 to-black" href="https://github.com"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 15 15" width="1.5rem" height="1.5rem"><path clip-rule="evenodd" fill-rule="evenodd" fill="currentColor" d="M7.49933 0.25C3.49635 0.25 0.25 3.49593 0.25 7.50024C0.25 10.703 2.32715 13.4206 5.2081 14.3797C5.57084 14.446 5.70302 14.2222 5.70302 14.0299C5.70302 13.8576 5.69679 13.4019 5.69323 12.797C3.67661 13.235 3.25112 11.825 3.25112 11.825C2.92132 10.9874 2.44599 10.7644 2.44599 10.7644C1.78773 10.3149 2.49584 10.3238 2.49584 10.3238C3.22353 10.375 3.60629 11.0711 3.60629 11.0711C4.25298 12.1788 5.30335 11.8588 5.71638 11.6732C5.78225 11.205 5.96962 10.8854 6.17658 10.7043C4.56675 10.5209 2.87415 9.89918 2.87415 7.12104C2.87415 6.32925 3.15677 5.68257 3.62053 5.17563C3.54576 4.99226 3.29697 4.25521 3.69174 3.25691C3.69174 3.25691 4.30015 3.06196 5.68522 3.99973C6.26337 3.83906 6.8838 3.75895 7.50022 3.75583C8.1162 3.75895 8.73619 3.83906 9.31523 3.99973C10.6994 3.06196 11.3069 3.25691 11.3069 3.25691C11.7026 4.25521 11.4538 4.99226 11.3795 5.17563C11.8441 5.68257 12.1245 6.32925 12.1245 7.12104C12.1245 9.9063 10.4292 10.5192 8.81452 10.6985C9.07444 10.9224 9.30633 11.3648 9.30633 12.0413C9.30633 13.0102 9.29742 13.7922 9.29742 14.0299C9.29742 14.2239 9.42828 14.4496 9.79591 14.3788C12.6746 13.4179 14.75 10.7025 14.75 7.50024C14.75 3.49593 11.5036 0.25 7.49933 0.25Z"></path></svg><span class="absolute opacity-0 group-hover:opacity-100 group-hover:text-gray-700 group-hover:text-sm group-hover:-translate-y-10 duration-700">GitHub</span></a></section><section class="flex justify-center items-center"><a href="https://scholar.google.com" class="group flex justify-center p-2 rounded-md drop-shadow-xl from-gray-800 bg-[#4285f4] text-white font-semibold hover:translate-y-3 hover:rounded-[50%] transition-all duration-500 hover:from-[#331029] hover:to-[#310413]"><svg xmlns="http://www.w3.org/2000/svg" width="1.5rem" height="1.5rem" role="img" viewBox="0 0 512 512" stroke-width="0" fill="currentColor" stroke="currentColor"><g transform="scale(1.5) translate(-100, -100)"><path d="M213 111l-107 94h69c5 45 41 64 78 67-7 18-4 27 7 39-43 1-103 26-103 67 4 45 63 54 92 54 38 1 81-19 90-54 4-35-10-54-31-71-23-18-28-28-21-40 15-17 35-27 39-51 2-17-2-28-6-43l45-38-1 16c-3 2-5 6-5 9v103c2 13 22 11 23 0V160c0-3-2-7-5-8v-25l16-16zm58 141c-61 10-87-87-38-99 56-11 83 86 38 99zm-5 73c60 13 61 63 10 78-44 9-82-4-81-30 0-25 35-48 71-48z"></path></g></svg><span class="absolute opacity-0 group-hover:opacity-100 group-hover:text-gray-700 group-hover:text-sm group-hover:-translate-y-10 duration-700 text-nowrap">Google Scholar</span></a></section><!--]--></div></div><!--]--></div><!----><div class="n-layout-sider__border"></div></aside><div class="__layout-light-1heifd0 n-layout n-layout--static-positioned" style=""><div class="n-layout-scroll-container" style=""><div class="n-layout-header __layout-header-light-1heifd0-b n-layout-header--static-positioned h-16" style=""><div class="APNavBar top" data-v-f3ac90ac><div class="APNavBarContent" data-v-f3ac90ac><div class="APNavBarContentTitle" data-v-f3ac90ac></div><div class="APNavBarContentMenu" data-v-f3ac90ac><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu" data-v-f3ac90ac data-v-557e00c5><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-557e00c5 data-v-6c058f83><!--[--><span data-v-6c058f83>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/publication" tabindex="0" data-v-557e00c5 data-v-6c058f83><!--[--><span data-v-6c058f83>Publication</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/research" tabindex="0" data-v-557e00c5 data-v-6c058f83><!--[--><span data-v-6c058f83>Research</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/team" tabindex="0" data-v-557e00c5 data-v-6c058f83><!--[--><span data-v-6c058f83>Team</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/teaching" tabindex="0" data-v-557e00c5 data-v-6c058f83><!--[--><span data-v-6c058f83>Teaching</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/cv" tabindex="0" data-v-557e00c5 data-v-6c058f83><!--[--><span data-v-6c058f83>CV</span><!--]--></a><!--]--><!--]--></nav></div></div><div class="APNavBarDivider" data-v-f3ac90ac><div class="APNavBarDivider-line" data-v-f3ac90ac></div></div></div></div><div class="__layout-light-1heifd0 n-layout-content n-layout n-layout--static-positioned" style=""><div class="n-layout-scroll-container" style="padding:0px;padding-left:0px;"><div class="container container-other"><div class="bg-white"><div class="mx-4 max-w-5xl pb-6 lg:px-8"><div class="text-base"><div class="sec-pub flex flex-col publication" data-v-ed60f995><div class="sec-pub-content flex flex-row justify-between items-center" data-v-ed60f995><div class="flex flex-nowrap items-stretch gap-0" data-v-ed60f995><div style="position:relative;" data-v-ed60f995><div><h1 id="selected-publications" tabindex="-1">ğŸ§‘â€ğŸ’» Selected Publications <a class="header-anchor" href="#selected-publications" aria-label="Permalink to &quot;:technologist: Selected Publications&quot;">â€‹</a></h1></div></div><div data-v-ed60f995><a class="__button-light-1heifd0-hlmmi n-button n-button--info-type n-button--medium-type my-8" tabindex="0" type="button" style="" href="https://scholar.google.com/citations?user=Ae2kRCEAAAAJ&amp;hl=zh-CN" data-v-ed60f995><!----><!----><span class="n-button__content"> Full List </span><div aria-hidden="true" class="n-base-wave"></div><!----><!----></a></div></div><div class="sort-by-what flex gap-1 items-stretch text-stone-400" data-v-ed60f995><span class="" data-v-ed60f995>Sorted by</span><div role="switch" aria-checked="false" class="n-switch __switch-light-1heifd0-m n-switch--round n-switch--rubber-band" tabindex="0" style="" data-v-ed60f995><div class="n-switch__rail" aria-hidden="true" style=""><div aria-hidden="true" class="n-switch__children-placeholder"><div class="n-switch__rail-placeholder"><div class="n-switch__button-placeholder"></div><!--[--> Tag <!--]--></div><div class="n-switch__rail-placeholder"><div class="n-switch__button-placeholder"></div><!--[--> Year <!--]--></div></div><div class="n-switch__button"><!----><div class="n-switch__checked"> Tag </div><div class="n-switch__unchecked"> Year </div></div></div></div></div></div><div class="sec-pub-filter flex flex-wrap gap-3 justify-start mb-3" data-v-ed60f995><!--[--><button class="__button-light-1heifd0-almmd n-button n-button--default-type n-button--medium-type n-button--dashed active active-tag" tabindex="0" type="button" style="" data-v-ed60f995><!----><!----><span class="n-button__content">ALL</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></button><button class="__button-light-1heifd0-almmd n-button n-button--default-type n-button--medium-type n-button--dashed active-tag" tabindex="0" type="button" style="" data-v-ed60f995><!----><!----><span class="n-button__content">2024</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></button><button class="__button-light-1heifd0-almmd n-button n-button--default-type n-button--medium-type n-button--dashed active-tag" tabindex="0" type="button" style="" data-v-ed60f995><!----><!----><span class="n-button__content">2023</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></button><button class="__button-light-1heifd0-almmd n-button n-button--default-type n-button--medium-type n-button--dashed active-tag" tabindex="0" type="button" style="" data-v-ed60f995><!----><!----><span class="n-button__content">2022</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></button><button class="__button-light-1heifd0-almmd n-button n-button--default-type n-button--medium-type n-button--dashed active-tag" tabindex="0" type="button" style="" data-v-ed60f995><!----><!----><span class="n-button__content">2021</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></button><button class="__button-light-1heifd0-almmd n-button n-button--default-type n-button--medium-type n-button--dashed active-tag" tabindex="0" type="button" style="" data-v-ed60f995><!----><!----><span class="n-button__content">2020</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></button><button class="__button-light-1heifd0-almmd n-button n-button--default-type n-button--medium-type n-button--dashed active-tag" tabindex="0" type="button" style="" data-v-ed60f995><!----><!----><span class="n-button__content">Before 2019</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></button><!--]--></div><div class="sec-pub-papers flex flex-col gap-3 text-justify break-words" data-v-ed60f995><!--[--><!--[--><h2 data-v-ed60f995>2024</h2><!--[--><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">arXiv</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chongjie Si<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>See Further for Parameter Efficient Fine-tuning by Standing on the Shoulders of Decomposition</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>arXiv</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>2407.05417</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2407.05417" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">arXiv</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chongjie Si<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xuehui Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xue Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhengqin Xu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qingyun Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jifeng Dai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yu Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>FLoRA: Low-Rank Core Space for N-dimension</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>arXiv</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>2405.14739</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2405.14739.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TMI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kailing Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuehao Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Dou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Efficient Deformable Tissue Reconstruction via Orthogonal Neural Plane</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Medical Imaging</span><span data-v-ed60f995>,Â </span></div><!----><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2312.15253.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TCSVT</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiazhong Cen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zekun Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dongsheng Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Tian<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Consensus Synergizes with Memory: A Simple Approach for Anomaly Segmentation in Urban Scenes</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Circuits and Systems for Video Technology</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>34(2), 1086-1097</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2111.15463.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TCSVT</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Danyang Tu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Sun<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiongkuo Min<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Changwen Chen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Un-Gaze: an Unified Transformer for Joint Gaze-Location and Gaze-Object Detection</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Circuits and Systems for Video Technology</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>34(5), 3271-3285</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2308.13857.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TCSVT</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Boxiang Yun<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Baiying Lei<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jieneng Chen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Song Qiu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qingli Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>SpecTr: Spectral Transformer for Microscopic Hyperspectral Pathology Image Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Circuits and Systems for Video Technology</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>34(6), 4610-4624</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://ieeexplore.ieee.org/document/10288474" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chongjie Si<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xuehui Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Milano, Italy</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2404.11981" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Tongkun Guan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xue Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xuehui Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Bridging Synthetic and Real Worlds for Pre-training Scene Text Detectors</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Milano, Italy</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2312.05286.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Tongkun Guan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chengyu Lin<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>PosFormer: Recognizing Complex Handwritten Mathematical Expression with Position Forest Transformer</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Milano, Italy</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2407.07764" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Duan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiongkuo Min<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Sijing Wu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>UniProcessor: A Text-induced Unified Low-level Image Processor</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Milano, Italy</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2407.20928" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zelin Peng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhengqin Xu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhilin Zeng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Tian<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Seattle, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2311.17112.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">BIBM</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhilin Zeng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zelin Peng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>DeCo-Net: Robust Multimodal Brain Tumor Segmentation via Decoupled Complementary Knowledge Distillation Model</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Lisbon, Portugal</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><!----></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">MICCAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kailing Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuehao Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Sikuang Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Dou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Marrakesh, Morocco</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2403.15124" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">MICCAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhilin Zeng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zelin Peng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Missing as Masking: Arbitrary Cross-modal Feature Reconstruction for Incomplete Multimodal Brain Tumor Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Marrakesh, Morocco</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><!----></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">AAAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zelin Peng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhengqin Xu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhilin Zeng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>SAM-PARSER: Fine-tuning SAM Efficiently by Parameter Space Reconstruction</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>AAAI Conference on Artificial Intelligence (AAAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vancouver, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2308.14604" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">AAAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Danning Lao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiazi Bu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Junchi Yan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>ViTree: Single-Path Neural Tree for Step-Wise Interpretable Fine-Grained Visual Categorization</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>AAAI Conference on Artificial Intelligence (AAAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vancouver, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2401.17050" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><div class="n-tag __tag-light-1heifd0-esc n-tag--strong n-tag--round w-full justify-center overflow-hidden hover:w-max" style="" data-v-ed60f995><!----><span class="n-tag__content">ORAL</span><!----><div class="n-tag__border" style=""></div></div><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">AAAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chongjie Si<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zekun Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xuehui Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Partial Label Learning with a Partner</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>AAAI Conference on Artificial Intelligence (AAAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vancouver, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2312.11034" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICASSP</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huayu Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zekun Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dongsheng Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Tian<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Domain-Adaptive Semantic Segmentation Emerges From Vision-Language Supervised Domain-Debiased Self-Training</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Seoul, Korea</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2024</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://ieeexplore.ieee.org/abstract/document/10447308" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><!--]--><!--]--><!--[--><h2 data-v-ed60f995>2023</h2><!--[--><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">arXiv</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiazhong Cen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiemin Fang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zanwei Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaopeng Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Tian<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Segment Anything in 3D with Radiance Fields</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>arXiv</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>2304.12308</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2304.12308.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">arXiv</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiazhong Cen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiemin Fang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaopeng Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Tian<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Segment Any 3D Gaussians</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>arXiv</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>2312.0086</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2312.00860.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TPAMI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zelin Peng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xuehui Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huayu Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiazhong Cen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dongsheng Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Tian<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>A Survey on Label-efficient Deep Image Segmentation: Bridging the Gap between Weak Supervision and Dense Prediction</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Pattern Analysis and Machine Intelligence</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>45(8), 9284-9305</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2207.01223.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TPAMI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuhui Xu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Cihang Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wenrui Dai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jieru Mei<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Hongkai Xiong<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Batch Normalization with Enhanced Linear Transformation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Pattern Analysis and Machine Intelligence</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>45(7), 9225-9232</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2011.14150.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TMM</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Duan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiongkuo Min<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuan Tian<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jae-Hyun Jung<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Develop then Rival: A Human Vision-Inspired Framework for Superimposed Image Decomposition</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Multimedia</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>25, 4267-4281</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://ieeexplore.ieee.org/document/9769950" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TMM</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuzhi Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lai-Man Po<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xuehui Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qiong Yan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yujia Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chun-Kit Wong<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chiu-Sing Pang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Weifeng Ou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wing-Yin Yu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Buhua Liu<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>ChildPredictor: A Child Face Prediction Framework with Disentangled Learning</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Multimedia</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>25, 3737-3752</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://ieeexplore.ieee.org/document/9749880" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">NeurIPS</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiazhong Cen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zanwei Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiemin Fang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dongsheng Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaopeng Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Tian<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Segment Anything in 3D with NeRFs</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Advances in Neural Information Processing Systems (NeurIPS)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>New Orleans, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/525d24400247f884c3419b0b7b1c4829-Paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/SJTU-DeepVisionLab/SegmentAnythingin3D" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zelin Peng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guanchun Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dongsheng Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Tian<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Computer Vision (ICCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Paris, France</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2303.07806" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Tongkun Guan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xue Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Feng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zekun Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Self-supervised Character-to-Character Distillation for Text Recognition</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Computer Vision (ICCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Paris, France</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2211.00288" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Danyang Tu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Sun<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Agglomerative Transformer for Human-Object Interaction Detection</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Computer Vision (ICCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Paris, France</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2308.08370" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">MICCAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kailing Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuehao Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vancouver, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2305.19906" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Peihao Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zanwei Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Shanxin Yuan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bingbing Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiaokang Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Weichao Qiu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vancouver, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2304.06287" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Tongkun Guan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chaochen Gu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jingzheng Tu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xue Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qi Feng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yudi Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Self-supervised Implicit Glyph Attention for Text Recognition</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vancouver, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2203.03382" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Duowen Chen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yunhao Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qingli Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lequan Yu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>MagicNet: Semi-Supervised Multi-Organ Segmentation via Magic-Cube Partition and Recovery</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vancouver, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2212.14310" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yunhao Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Duowen Chen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qingli Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vancouver, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/2305.00673" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">AAAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xintian Mao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yiming Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Fengze Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qingli Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Intriguing Findings of Frequency Selection for Image Deblurring</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>AAAI Conference on Artificial Intelligence (AAAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Washington, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2023</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2111.11745.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><!--]--><!--]--><!--[--><h2 data-v-ed60f995>2022</h2><!--[--><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">NatComm</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Michela Antonelli<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Annika Reinke<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Spyridon Bakas<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Keyvan Farahani<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Annette Kopp-Schneider<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bennett A. Landman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Geert Litjens<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bjoern Menze<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Olaf Ronneberger<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ronald M. Summers<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bram van Ginneken<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Michel Bilello<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Patrick Bilic<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Patrick F. Christ<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Richard K. G. Do<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Marc J. Gollub<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Stephan H. Heckers<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Henkjan Huisman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->William R. Jarnagin<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Maureen K. McHugo<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Sandy Napel<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jennifer S. Goli Pernicka<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kawal Rhode<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Catalina Tobon-Gomez<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Eugene Vorontsov<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->James A. Meakin<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Sebastien Ourselin<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Manuel Wiesenfarth<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Pablo Arbelaez<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Byeonguk Bae<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Sihong Chen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Laura Daza<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jianjiang Feng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Baochun He<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Fabian Isensee<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuanfeng Ji<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Fucang Jia<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Namkug Kim<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ildoo Kim<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dorit Merhof<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Akshay Pai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Beomhee Park<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Mathias Perslev<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ramin Rezaiifar<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Oliver Rippel<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ignacio Sarasua<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jaemin Son<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Christian Wachinger<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Liansheng Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yingda Xia<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Daguang Xu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhanwei Xu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yefeng Zheng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Amber L. Simpson<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lena Maier-Hein<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->M. Jorge Cardoso<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>The Medical Segmentation Decathlon</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Nature Communications</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>13(4128)</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2106.05735.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TCSVT</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Shunyu Yao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zanwei Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bin Ji<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Poxture: Human Posture Imitation Using Neural Texture</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Circuits and Systems for Video Technology</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>32(12), 8537 - 8549</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://ieeexplore.ieee.org/document/9829868" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TMI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->David Dreizin<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Fengze Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>External Attention Assisted Multi-Phase Splenic Vascular Injury Segmentation with Limited Data</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Medical Imaging</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>41(6), 1346-1357</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/External_Attention.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">NeurIPS</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Danyang Tu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Sun<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiongkuo Min<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Video-based Human-Object Interaction Detection from Tubelet Tokens</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Advances in Neural Information Processing Systems (NeurIPS)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>New Orleans, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2206.01908.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Feng Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Wei<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>CP2: Copy-Paste Contrastive Pretraining for Semantic Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Tel-Aviv, Israel</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2203.11709.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/SJTU-DeepVisionLab/CP2" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Danyang Tu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiongkuo Min<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Duan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guodong Guo<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Iwin: Human-Object Interaction Detection via Transformer with Irregular Windows</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Tel-Aviv, Israel</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2203.10537.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kai Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yingyi Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chuhan Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Tao Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ruixin Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Shouhong Ding<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Jia<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>BÃ©zierPalm: A Free Lunch for Palmprint Recognition</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Tel-Aviv, Israel</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2203.05703.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ACM MM</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yunhao Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhenbo Yu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yucheng Zhu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bingbing Ni<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Skeleton2Humanoid: Animating Simulated Characters for Physically-plausible Motion In-betweening</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>ACM International Conference on Multimedia (ACM MM)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Lisbon, Portugal</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2210.04294.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ACM MM</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Duan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiongkuo Min<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Danyang Tu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jing Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Saliency in Augmented Reality</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>ACM International Conference on Multimedia (ACM MM)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Lisbon, Portugal</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2204.08308.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xuehui Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kai Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ruixin Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Shouhong Ding<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>ContrastMask: Contrastive Learning to Segment Every Thing</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>New Orleans, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_ContrastMask_Contrastive_Learning_To_Segment_Every_Thing_CVPR_2022_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/SJTU-DeepVisionLab/ContrastMask" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Danyang Tu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiongkuo Min<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Duan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guodong Guo<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>End-to-End Human-Gaze-Target Detection with Transformers</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>New Orleans, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tu_End-to-End_Human-Gaze-Target_Detection_With_Transformers_CVPR_2022_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICLR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jinghao Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Wei<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Cihang Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Tao Kong<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>iBOT: Image BERT Pre-Training with Online Tokenizer</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Learning Representations (ICLR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Lisbon, Portugal</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2022</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2111.07832.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><!--]--><!--]--><!--[--><h2 data-v-ed60f995>2021</h2><!--[--><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TPAMI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yilu Guo<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kai Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bo Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Deep Differentiable Random Forests for Age Estimation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Pattern Analysis and Machine Intelligence</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>43(2), 404-419</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2021</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/1907.10665" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/shenwei1231/caffe-DeepDecisionForest" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TMI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Peng Tang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot K. Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Learning Inductive Attention Guidance for Partially Supervised Pancreatic Ductal Adenocarcinoma Prediction</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Medical Imaging</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>43(2), 2723-2735</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2021</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/IAG-NET-online.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">NeurIPS</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qihang Yu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yingda Xia<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yutong Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yongyi Lu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Glance-and-Gaze Vision Transformer</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Advances in Neural Information Processing Systems (NeurIPS)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Sydney, Australia</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2021</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://proceedings.neurips.cc/paper/2021/file/6c524f9d5d7027454a783c841250ba71-Paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yunhao Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhongpai Gao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yucheng Zhu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guodong Guo<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Looking Here or There? Gaze Following in 360-Degree Images</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Computer Vision (ICCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Montreal, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2021</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Looking_Here_or_There_Gaze_Following_in_360-Degree_Images_ICCV_2021_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Hao Ding<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Deeply Shape-guided Cascade for Instance Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Nashville, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2021</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/1911.11263.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/SJTU-DeepVisionLab/DSC" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><div class="n-tag __tag-light-1heifd0-esc n-tag--strong n-tag--round w-full justify-center overflow-hidden hover:w-max" style="" data-v-ed60f995><!----><span class="n-tag__content">ORAL</span><!----><div class="n-tag__border" style=""></div></div><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yi Fang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jiapeng Tang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wang Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiao Gu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Li Song<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Guangtao Zhai<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Dual Attention Guided Gaze Target Detection in the Wild</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Nashville, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2021</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Fang_Dual_Attention_Guided_Gaze_Target_Detection_in_the_Wild_CVPR_2021_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICLR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chen Wei<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>CO2: Consistent Contrast for Unsupervised Visual Representation Learning</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Learning Representations (ICLR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vienna, Austria</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2021</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openreview.net/pdf?id=U4XLJhqwNF1" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICLR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yingwei Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qihang Yu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Mingxing Tan<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jieru Mei<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Peng Tang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Cihang Xie<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Shape-Texture Debiased Neural Network Training</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Learning Representations (ICLR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Vienna, Austria</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2021</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openreview.net/pdf?id=Db4yerZTYkz" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><!--]--><!--]--><!--[--><h2 data-v-ed60f995>2020</h2><!--[--><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TPAMI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Peng Tang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xinggang Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Song Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wenyu Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>PCL: Proposal Cluster Learning for Weakly Supervised Object Detection</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Pattern Analysis and Machine Intelligence</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>42(1), 176-191</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2020</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/1807.03342" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">IROS</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Daniil Pakhomov<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Nassir Navab<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Towards Unsupervised Learning for Instrument Segmentation in Robotic Surgery with Cycle-Consistent Adversarial Networks</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Las Vegas, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2020</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2007.04505.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><div class="n-tag __tag-light-1heifd0-esc n-tag--strong n-tag--round w-full justify-center overflow-hidden hover:w-max" style="" data-v-ed60f995><!----><span class="n-tag__content">ORAL</span><!----><div class="n-tag__border" style=""></div></div><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yingda Xia<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yi Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Fengze Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Synthesize then Compare: Detecting Failures and Anomalies for Semantic Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Glasgow, UK</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2020</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2003.08440.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/YingdaXia/SynthCP" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">MICCAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yingda Xia<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Qihang Yu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Detecting Pancreatic Adenocarcinoma in Multi-phase CT Scans via Alignment Ensemble</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Lima, Peru</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2020</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2003.08441.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">MICCAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Shuhao Fu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yongyi Lu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Domain Adaptive Relational Reasoning for 3D Multi-Organ Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Lima, Peru</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2020</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/2005.09120.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><div class="n-tag __tag-light-1heifd0-esc n-tag--strong n-tag--round w-full justify-center overflow-hidden hover:w-max" style="" data-v-ed60f995><!----><span class="n-tag__content">ORAL</span><!----><div class="n-tag__border" style=""></div></div><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xu Wei<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Fengze Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Jieneng Chen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Deep Distance Transform for Tubular Structure Segmentation in CT Scans</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Seattle, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2020</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Deep_Distance_Transform_for_Tubular_Structure_Segmentation_in_CT_Scans_CVPR_2020_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">WACV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhishuai Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bo Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Robust Face Detection via Learning Small Faces on Hard Images</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Winter Conference on Applications of Computer Vision (WACV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Colorado, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2020</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/1811.11662.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/bairdzhang/smallhardface" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">WACV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yunhan Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ye Tian<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Charless Fowlkes<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Towards Resisting Large Data Variations via Introspective Learning</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Winter Conference on Applications of Computer Vision (WACV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Colorado, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2020</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/1805.06447.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><!--]--><!--]--><!--[--><h2 data-v-ed60f995>Before 2019</h2><!--[--><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">arXiv</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Huiyu Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chenxi Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Micro-Batch Training with Batch-Channel Normalization and Weight Standardization</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>arXiv</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>1903.1052</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2019</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/1903.10520.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">MedIA</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Seyoun Park<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot K. Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Abdominal multi-organ segmentation with organ-attention networks and statistical fusion</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Medical Image Analysis</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>55, 88-102</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2019</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/1804.08414" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">WACV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Peng Tang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Song Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot K. Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Semi-Supervised 3D Multi-Organ Segmentation via Deep Multi-Planar Co-Training</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Winter Conference on Applications of Computer Vision (WACV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Hawaii, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2019</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/1804.02586.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ECCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhishuai Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bo Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Deep Co-Training for Semi-Supervised Image Recognition</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>European Conference on Computer Vision (ECCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Munich, Germany</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Siyuan_Qiao_Deep_Co-Training_for_ECCV_2018_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICML</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhishuai Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bo Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Gradually Updated Neural Networks for Large-Scale Image Recognition</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Machine Learning (ICML)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Stockholm, Sweden</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="http://proceedings.mlr.press/v80/qiao18b/qiao18b.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/joe-siyuan-qiao/GUNN" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yilu Guo<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kai Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bo Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Deep Regression Forests for Age Estimation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Salt Lake City, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Regression_Forests_CVPR_2018_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/shenwei1231/caffe-DeepRegressionForests" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><div class="n-tag __tag-light-1heifd0-esc n-tag--strong n-tag--round w-full justify-center overflow-hidden hover:w-max" style="" data-v-ed60f995><!----><span class="n-tag__content">SPOTLIGHT</span><!----><div class="n-tag__border" style=""></div></div><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chenxi Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Few-Shot Image Recognition by Predicting Parameters from Activations</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Salt Lake City, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Qiao_Few-Shot_Image_Recognition_CVPR_2018_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/joe-siyuan-qiao/FewShot-CVPR" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhishuai Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Cihang Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bo Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Single-Shot Object Detection with Enriched Semantics</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Salt Lake City, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single-Shot_Object_Detection_CVPR_2018_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/bairdzhang/des" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><div class="n-tag __tag-light-1heifd0-esc n-tag--strong n-tag--round w-full justify-center overflow-hidden hover:w-max" style="" data-v-ed60f995><!----><span class="n-tag__content">ORAL</span><!----><div class="n-tag__border" style=""></div></div><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">MICCAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Peng Tang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot K. Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Granada, Spain</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/1804.02595.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">3DV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhuotun Zhu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yingda Xia<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot K Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on 3D Vision (3DV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Verona, Italy</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/1712.00201" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">IJCAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kai Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Shanghua Gao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dandan Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ming-Ming Cheng<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Hi-Fi: Hierarchical Feature Integration for Skeleton Detection</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Joint Conference on Artificial Intelligence (IJCAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Stockholm, Sweden</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/abs/1801.01849" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">PRL</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chenting Du<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuan Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dan Zeng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhijiang Zhang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Bag of Shape Features with A Learned Pooling Function for Shape Recognition</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Pattern Recognition Letters</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>106(15), 33â€“40</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2018</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/bosf_lp.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TIP</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kai Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuan Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Image Processing</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>26(11), 5298-5311</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2017</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="http://arxiv.org/abs/1609.03659" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="http://kaiz.xyz/deepsk" data-v-ed60f995><!----><!----><span class="n-button__content">Project Page</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/zeakey/skeleton" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/sk1491.tar" data-v-ed60f995><!----><!----><span class="n-button__content">SK-LARGE Dataset</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">NIPS</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kai Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yilu Guo<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Label Distribution Learning Forests</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Advances in Neural Information Processing Systems (NIPS)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Long Beach, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2017</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://papers.nips.cc/paper/2017/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/posterldlforests.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">POSTER</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/shenwei1231/caffe-LDLForests" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICCVW</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Christopher Funk<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Seungkyu Lee<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Martin R. Oswald<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Stavros Tsogkas<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Andrea Cohen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Sven Dickinson<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yanxi Liu<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>2017 ICCV Challenge: Detecting Symmetry in the Wild</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Computer Vision Workshop (ICCVW)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Venice, Italy</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2017</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w24/Funk_2017_ICCV_Challenge_ICCV_2017_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bin Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuan Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Multi-stage Multi-recursive-input Fully Convolutional Networks for Neuronal Boundary Detection</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Computer Vision (ICCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Venice, Italy</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2017</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Shen_Multi-Stage_Multi-Recursive-Input_Fully_ICCV_2017_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/BinWang-shu/M2FCN" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Siyuan Qiao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Weichao Qiu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chenxi Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Computer Vision (ICCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Venice, Italy</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2017</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Qiao_ScaleNet_Guiding_Object_ICCV_2017_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/joe-siyuan-qiao/ScaleNet" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><div class="n-tag __tag-light-1heifd0-esc n-tag--strong n-tag--round w-full justify-center overflow-hidden hover:w-max" style="" data-v-ed60f995><!----><span class="n-tag__content">ORAL</span><!----><div class="n-tag__border" style=""></div></div><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICIP</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wenjing Gao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuan Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dan Zeng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhijiang Zhang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Shape Recognition by Bag of Contour Fragments with A Learned Pooling Function</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Image Processing (ICIP)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Beijing, China</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2017</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/bocf_lp.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">MICCAI</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuyin Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Lingxi Xie<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Elliot Fishman<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Alan Yuille<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Quebec City, Canada</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2017</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://arxiv.org/pdf/1612.08230.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/198808xc/OrganSegC2F" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">PR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zihao Hu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhijiang Zhang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Multiple Instance Subspace Learning via Partial Random Projection Tree for Local Reflection Symmetry in Natural Images</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Pattern Recognition</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>52(4), 306-316</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2016</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/pr_symmetrydetection.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/pr_curve_data.zip" data-v-ed60f995><!----><!----><span class="n-button__content">PR-curve Data</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/wh-symmax.zip" data-v-ed60f995><!----><!----><span class="n-button__content">WH-SYMMAX dataset</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Kai Zhao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yuan Jiang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhijiang Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Las Vegas, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2016</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/deepskeleton_final.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/supplementarymaterial_final.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">Supplementary Material</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/zeakey/DeepSkeleton" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/pr-curve.zip" data-v-ed60f995><!----><!----><span class="n-button__content">PR-curve Data</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/sk506.zip" data-v-ed60f995><!----><!----><span class="n-button__content">SK-SMALL Dataset</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zheng Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Chenquan Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Cong Yao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wenyu Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Multi-Oriented Text Detection with Fully Convolutional Networks</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Las Vegas, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2016</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Zhang_Multi-Oriented_Text_Detection_CVPR_2016_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xinggang Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhijiang Zhang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>DeepContour: A Deep Convolutional Feature Learned by Positive-sharing Loss for Contour Detection</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Boston, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2015</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Shen_DeepContour_A_Deep_2015_CVPR_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://github.com/shenwei1231/DeepContour" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/bsds_result.zip" data-v-ed60f995><!----><!----><span class="n-button__content">Pre-computed Result on BSDS500</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/bsds_pr.zip" data-v-ed60f995><!----><!----><span class="n-button__content">PR-curve Data</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zheng Zhang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Cong Yao<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Symmetry-Based Text Line Detection in Natural Scenes</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Boston, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2015</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Zhang_Symmetry-Based_Text_Line_2015_CVPR_paper.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICASSP</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Shifu Zhou<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dan Zeng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhijiang Zhang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Unusual Event Detection in Crowded Scenes by Trajectory Analysis</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Brisbane, Australia</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2015</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">TCYB</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ke Deng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Tommer Leyvand<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Baining Guo<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhuowen Tu<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Exemplar-based Human Action Pose Correction</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Trans. Cybernetics</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>44(7), 1053-1066</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2014</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/posecorrection.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ACCV</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Rui Lei<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Dan Zeng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhijiang Zhang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Regularity Guaranteed Human Pose Correction</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Asian Conference on Computer Vision (ACCV)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Singapore</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2014</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/rgposecorrectionfinal.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">PR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Yan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Hongyuan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Longin Jan Latecki<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Shape Clustering: Common Structure Discovery</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Pattern Recognition</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>46(2), 539-550</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2013</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/shape_clustering_common_structure_discovery.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/shape_clustering_cs.zip" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">SCIS</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xingwei Yang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Longin Jan Latecki<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Skeleton Pruning as Trade-off between Skeleton Simplicity and Reconstruction Error</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>SCIENCE CHINA Information Sciences</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>56(4), 1-14</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2013</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/skeleton_pruning_as_trade-off_between_skeleton_simplicity_and_reconstruction_error.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/skelpruningtradeoff.zip" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">FG</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ying Li<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xun Shi<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhijiang Zhang<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Ensemble of Randomized Linear Discriminant Analysis for Face Recognition with Single Sample Per Person</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference Automatic Face and Gesture Recognition (FG)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Shanghai, China</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2013</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/ensemble_of_randomized_linear_discriminant_analysis_for_face_recognition_with_single_sample_per_person/pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">CVPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Ke Deng<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Tommer Leyvand<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Baining Guo<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Zhuowen Tu<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Exemplar-based Human Action Pose Correction and Tagging</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Providence, USA</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2012</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/posecorrection_cvpr.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">PR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Rong Hu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Hongyuan Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Longin Jan Latecki<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Skeleton Growing and Pruning with Bending Potential Ratio</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>Pattern Recognition</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>44(2), 196-209</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2011</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/skeleton_growing_and_pruning_with_bending_potential_ratio.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/shenskeletonpruningbpr.zip" data-v-ed60f995><!----><!----><span class="n-button__content">CODE</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><div class="sec-pub-papers-item flex" data-v-ed60f995><div class="paper-tags min-w-16 max-w-16 mr-2 flex flex-col gap-2 p-1" data-v-ed60f995><!----><div class="n-tag __tag-light-1heifd0-isc n-tag--strong n-tag--round w-full justify-center" style="" data-v-ed60f995><!----><span class="n-tag__content">ICPR</span><!----><div class="n-tag__border" style=""></div></div></div><div class="paper-info text-lg" data-v-ed60f995><div class="pub-authors inline" data-v-ed60f995><!--[--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Bo Wang<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="addBold" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wei Shen<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Wenyu Liu<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xinge You<!--]--><!----></div></span><span data-v-ed60f995>,Â </span><!--]--><!--[--><span class="" data-v-ed60f995><div class="n-badge __badge-light-1heifd0-d" style="" data-v-ed60f995><!--[-->Xiang Bai<!--]--><!----></div></span><span data-v-ed60f995>.Â </span><!--]--><!--]--></div><div class="pub-title inline text-[#1661AB]" data-v-ed60f995><span data-v-ed60f995>Shape Classification Using Tree Unions</span><span data-v-ed60f995>.Â </span></div><div class="pub-publish inline" data-v-ed60f995><span class="font-semibold" data-v-ed60f995>International Conference on Pattern Recognition (ICPR)</span><span data-v-ed60f995>,Â </span></div><div class="pub-extra inline" data-v-ed60f995><span data-v-ed60f995>Istanbul, Turkey</span><span data-v-ed60f995>,Â </span></div><div class="pub-year inline" data-v-ed60f995><span data-v-ed60f995>2010</span><span data-v-ed60f995>.Â </span></div><div class="pub-links flex flex-row gap-2" data-v-ed60f995><!--[--><span data-v-ed60f995><a class="__button-light-1heifd0-bltmi n-button n-button--info-type n-button--tiny-type n-button--ghost hover:text-black hover:font-semibold" tabindex="0" type="button" style="" href="/files/shape_classification_using_tree_unions.pdf" data-v-ed60f995><!----><!----><span class="n-button__content">PDF</span><div aria-hidden="true" class="n-base-wave"></div><div aria-hidden="true" class="n-button__border" style=""></div><div aria-hidden="true" class="n-button__state-border" style=""></div></a></span><!--]--></div></div></div><!--]--><!--]--><!--]--></div></div></div></div></div></div></div></div><div class="n-layout-footer __layout-footer-light-1heifd0-b n-layout-footer--absolute-positioned" style=""></div></div></div></div></div></div></div><!--[--><!--]--><!--]--></div>
    
    
  </body>
</html>